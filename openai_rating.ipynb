{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('INITIAL ANNOTATED FILE')\n",
    "# Columns: issue_id\tcomment_id\ttbdf\tcomment_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tbdf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_ai_token = OPEN_AI_TOKEN\n",
    "open_ai_organization = OPEN_AI_ORGANIZATION_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a token: https://platform.openai.com/account/api-keys\n",
    "\n",
    "from getpass import getpass\n",
    "\n",
    "OPENAI_API_KEY = getpass()\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = open_ai_token\n",
    "\n",
    "OPENAI_ORGANIZATION = getpass()\n",
    "\n",
    "os.environ[\"OPENAI_ORGANIZATION\"] = open_ai_organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"The following are the definitions of uncivil tones in GitHub comments:\n",
    "\n",
    "Bitter frustration: when someone expresses strong frustration.\n",
    "\n",
    "Impatience: participants might demonstrate impatience when they express a feeling that it is taking too long to solve a problem, understand a solution, or answer a question.\n",
    "\n",
    "Irony: contributors used expressions that usually signify the opposite in a mocking or blaming tone.\n",
    "\n",
    "Insulting: Insulting remarks directed at another person.\n",
    "\n",
    "Mocking: when a discussion participant is making fun of someone else, usually because that person has made a mistake.\n",
    "\n",
    "Threat: contributors put a condition impacting the result of another discussion participant or that personâ€™s career.\n",
    "\n",
    "Vulgarity: using profanity or language that is not considered proper.\n",
    "\n",
    "Entitlement: expecting special privileges, attention, or resources without regard for the norms of collaboration and respect.\n",
    "\n",
    "Identity attack/Name-Calling: Race, Religion, Nationality, Gender, Sexual-oriented, or any other kind of attacks and mean/offensive words directed at someone or a gorup of people.\n",
    "\n",
    "Nan: If tone is civil.\n",
    "____________________________________________________\n",
    "\n",
    "An annotator annotated the following comment as {annotated}.\n",
    "\n",
    "Comment: {sentence}.\n",
    "\n",
    "On a Likert chart on a scale 5, how do you rate the annotation? \n",
    "\n",
    "Answer: Let's think step by step. Write one word: the Likert chart rating.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"annotated\", \"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = OpenAI()\n",
    "llm = ChatOpenAI(model_name='gpt-4')\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "annotated = \"Entitlement\"\n",
    "\n",
    "sentence = \"Please learn how to read and stop spamming the `Vuetify 3.0 Release CHECKLIST` issue ticket.\\\n",
    "I didn't subscribe it to read wining people screaming for ETA without really helping the project and ignoring text warnings.\"\n",
    "\n",
    "question = {\"annotated\": annotated,\n",
    "            \"sentence\": sentence}\n",
    "\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4_ratings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    try:\n",
    "        question = {\"annotated\": row['tbdf'],\n",
    "                    \"sentence\": row['comment_body']}\n",
    "        output = llm_chain.run(question)\n",
    "        gpt_4_ratings.append(output)\n",
    "        print(index, output)\n",
    "    except:\n",
    "        gpt_4_ratings.append(\"Error\")\n",
    "        print(index, \"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary with the three lists\n",
    "dict = {'rating': gpt_4_ratings}  \n",
    "\n",
    "# create a Pandas DataFrame from the dictionary\n",
    "df1 = pd.DataFrame(dict) \n",
    "    \n",
    "# write the DataFrame to a CSV file\n",
    "df1.to_csv('rating.csv') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
